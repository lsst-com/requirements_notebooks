{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVV-T1278: Relative Astrometric Performance\n",
    "\n",
    "**Written By: Bryce Kalmbach**\n",
    "\n",
    "**Last updated: 08-06-2019**\n",
    "\n",
    "**Tested on Stack Version: w_2019_29**\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "[OSS-REQ-0388](https://docushare.lsst.org/docushare/dsweb/Get/LSE-030#page=66)\n",
    "\n",
    "1. For all pairs of sources separated by ~5 arcminutes median error in these measurements is <= 10 milliarcseconds.\n",
    "\n",
    "2. No more than 10% of the source pairs separated by ~5 arcminutes have separation errors greater than 20 milliarcseconds.\n",
    "\n",
    "3. For all pairs of sources separated by ~20 arcminutes median error in these measurements is <= 10 milliarcseconds.\n",
    "\n",
    "4. No more than 10% of the source pairs separated by ~20 arcminutes have separation errors greater than 20 milliarcseconds.\n",
    "\n",
    "5. For all pairs of sources separated by ~200 arcminutes median error in these measurements is <= 15 milliarcseconds.\n",
    "\n",
    "6. No more than 10% of the source pairs separated by ~200 arcminutes have separation errors greater than 30 milliarcseconds.\n",
    "\n",
    "## Proposed Test Case:\n",
    "\n",
    "1. Image an average field. Repeat at different airmasses.\n",
    "\n",
    "2. Run source detection and astrometric measurement on images from step 1\n",
    "\n",
    "3. Calculate the separations between all sources detected in step 2\n",
    "\n",
    "4. Compare source separations from step 3. Calculate RMS for each pair across set of visits.\n",
    "\n",
    "5. Examine distribution of source separation RMS from step 4 for all pairs of sources separated by ~ 5 arcminutes. Verify that the median in these measurements is <= 10 milliarcseconds\n",
    "\n",
    "6. Verify that no more than 10% of the source pairs separated by ~ 5 arcminutes have separation RMS greater than 20 milliarcseconds\n",
    "\n",
    "7. Examine distribution of source separation RMS from step 4 for all pairs of sources separated by ~ 20 arcminutes. Verify that the median in these measurements is <= 10 milliarcseconds\n",
    "\n",
    "8. Verify that no more than 10 percent of source pairs separated by ~ 20 arcminutes have source separation RMS greater than 20 milliarcseconds\n",
    "\n",
    "9. Examine distribution of source separation RMS from step 4 for all pairs separated by ~ 200 arcminutes. Verify that the median in these measurements is <= 15 milliarcseconds.\n",
    "\n",
    "10. Verify that no more than 10 percent of sources separated by ~ 200 arcminutes have source separation RMS greater than 30 milliarcseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.persistence import Butler\n",
    "import lsst.daf.persistence as daf_persistence\n",
    "from lsst.afw.table import MultiMatch\n",
    "\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from astropy import units as u\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import lsst.verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our plots nice and readable\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for testing\n",
    "\n",
    "* `test_bandpass`: The notebook will set up to test astrometry in this bandpass against Gaia\n",
    "\n",
    "* `mag_lims`: Keep stars with magnitudes in between `[bright_limit, faint_limit]`\n",
    "\n",
    "* `num_visits`: Number of visits to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bandpass = 'HSC-R'\n",
    "\n",
    "mag_lims = [17.5, 21.5]\n",
    "\n",
    "num_visits = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify HSC Data to use\n",
    "\n",
    "We want to get data from a single visit for this requirement so we choose a visit from the HSC Wide dataset. https://hsc-release.mtk.nao.ac.jp/doc/index.php/database/ has info \n",
    "on which tracts are included in the Wide data. We have an hdf5 file with the visit data located at `/project/danielsf/valid_hsc_visit_extent.h5`. From this file we load in a visit to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a butler for the HSC Wide data\n",
    "depth = 'WIDE'\n",
    "band = test_bandpass\n",
    "butler = daf_persistence.Butler('/datasets/hsc/repo/rerun/DM-13666/%s/'%(depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('/project/danielsf/valid_hsc_visit_extent.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the hdf5 data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsc_data_df = pd.DataFrame([])\n",
    "for key in list(f.keys()):\n",
    "    if key == 'filter':\n",
    "        hsc_data_df[key] = np.array(f[key][()], dtype=str)\n",
    "    else:\n",
    "        hsc_data_df[key] = f[key][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsc_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random visit from the data observed in the desired test bandpass and find a visit which is centered near the same area so we have overlap for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_visits = np.unique(hsc_data_df.query('filter == \"%s\"' % test_bandpass)['visit'].values)\n",
    "rand_state = np.random.RandomState(123)\n",
    "test_visit_1 = rand_state.choice(unique_visits)\n",
    "print(test_visit_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will search for another visit that overlaps the test visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_visit_df = hsc_data_df.query('visit == %i and ccd == %i' % (test_visit_1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_min = test_visit_df['ra_min'].iloc[0]\n",
    "ra_max = test_visit_df['ra_max'].iloc[0]\n",
    "dec_min = test_visit_df['dec_min'].iloc[0]\n",
    "dec_max = test_visit_df['dec_max'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_visits_df = hsc_data_df.query(str('ra_center > %f and ra_center < %f ' +\n",
    "                                          'and dec_center > %f and dec_center < %f') % (ra_min, ra_max,\n",
    "                                                                                        dec_min, dec_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_visits = overlap_visits_df.query('filter == \"%s\" and ccd == %i' % (test_bandpass, 0))['visit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_visit_2 = overlap_visits[overlap_visits != test_visit_1][0]\n",
    "print(test_visit_1, test_visit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define methods to create a matched catalog for sources and create `objects` made up of individual sources detected in a single visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_catalog(subset, visit_list):\n",
    "    \"\"\"\n",
    "    Create a matched catalog from a subset with observations in the bandpasses listed.\n",
    "    \"\"\"\n",
    "\n",
    "    matched_cat = None\n",
    "    calexps = {}          \n",
    "\n",
    "    for data_ref in subset:\n",
    "        data_id = data_ref.dataId\n",
    "        if data_id['visit'] not in visit_list:\n",
    "            continue\n",
    "        try:\n",
    "            src_cat = data_ref.get('src')\n",
    "        except:\n",
    "            print('Error in Visit #%i, CCD #%i. Skipping it.' % (data_id['visit'], data_id['ccd']))\n",
    "            continue\n",
    "        calexps[data_id['visit']] = data_ref.get('calexp')\n",
    "        if matched_cat is None:\n",
    "            id_fmt = {'visit':np.int64}\n",
    "            matched_cat = MultiMatch(src_cat.schema, id_fmt)\n",
    "        matched_cat.add(src_cat, data_id)\n",
    "        \n",
    "    final_catalog = matched_cat.finish()\n",
    "    \n",
    "    return final_catalog, calexps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that Gen 2 butler does not like numpy ints\n",
    "subset = butler.subset('src', filter='HSC-R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_catalog, calexps = get_matched_catalog(subset, [test_visit_1, test_visit_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of HSC Sources\n",
    "len(final_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the columns we need going forward and convert to pandas dataframe\n",
    "final_catalog = final_catalog.asAstropy()\n",
    "final_catalog = final_catalog[['id', 'coord_ra', 'coord_dec', 'base_PsfFlux_instFlux', 'object', 'visit']]\n",
    "final_catalog = final_catalog.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in magnitude information for cuts\n",
    "mag = []\n",
    "for obj_row in final_catalog.values:\n",
    "    calib = calexps[obj_row[-1]].getPhotoCalib()\n",
    "    mag.append(calib.instFluxToMagnitude(obj_row[-3]))\n",
    "final_catalog['mag'] = mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make cuts based upon magnitude set at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsc_final_df = final_catalog.query('mag > %f and mag < %f' % (mag_lims[0], mag_lims[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsc_sources_coords = SkyCoord(hsc_final_df['coord_ra'].values*u.rad, hsc_final_df['coord_dec'].values*u.rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plot all the sources that we've kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(hsc_sources_coords.ra.deg, hsc_sources_coords.dec.deg, s=8, lw=0)\n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('dec (deg)')\n",
    "plt.title('HSC Sources in Matched Catalog')\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(ax.get_xticks()[1::2]) # Clean up ticks in RA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find separations in all pairs of sources\n",
    "\n",
    "To get source separations to compare we need to only keep the objects that appear in both visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pairs of all objects with detections in both filters\n",
    "# Faster to use numpy array than loop over pandas df\n",
    "# Currently keeps only the objects present in all visits\n",
    "unique, counts = np.unique(hsc_final_df['object'].values, return_counts=True)\n",
    "in_all = unique[np.where(counts == num_visits)[0]]\n",
    "num_unique_objects = len(in_all)\n",
    "print(\"Number of Objects present in all visits: %i\" % num_unique_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_catalog = final_catalog[final_catalog['object'].isin(in_all)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a catalog for each visit so we can compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_visit_1 = keep_catalog.query('visit == %i' % test_visit_1)\n",
    "keep_visit_2 = keep_catalog.query('visit == %i' % test_visit_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed things up we randomly select `use_objects` number of objects to calculate the separations. It takes a long time to calculate the separations for *all* possible pairs of objects in the visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_objects = 300\n",
    "rand_state = np.random.RandomState(98)\n",
    "pairs_list = list(combinations(rand_state.choice(np.arange(num_unique_objects), \n",
    "                                                 size=use_objects, replace=False),\n",
    "                               2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pairwise_separations(cat_ra, cat_dec, pairs_list, cat_units):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the separation between pairs of objects found in a catalog.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    cat_ra: list of floats\n",
    "        The ra coordinates of the catalog objects in units given by cat_units\n",
    "        \n",
    "    cat_dec: list of floats\n",
    "        The dec coordinates of the catalog objects in units given by cat_units\n",
    "        \n",
    "    pairs_list: list of len-2 lists\n",
    "        The indices of pairs of catalogs objects for which to calculate the separations\n",
    "        \n",
    "    cat_units: astropy Unit\n",
    "        The units of the coordinates\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cat_seps: list of floats\n",
    "        The separations of the pairs of objects defined in pairs_list given in arcsec\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_1_locs = SkyCoord(cat_ra*cat_units, cat_dec*cat_units)\n",
    "    cat_seps = []\n",
    "    j = 0\n",
    "    for pair_1, pair_2 in pairs_list:\n",
    "        if j % 5000 == 0:\n",
    "            print('Calculating Separation %i out of %i' % (j, len(pairs_list)))\n",
    "        pair_seps = cat_1_locs[pair_1].separation(cat_1_locs[pair_2]).arcsec\n",
    "        cat_seps.append(pair_seps)\n",
    "        j += 1\n",
    "        \n",
    "    return cat_seps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Separations for Visit %i\" % test_visit_1)\n",
    "visit_1_seps = calc_pairwise_separations(keep_visit_1['coord_ra'], keep_visit_1['coord_dec'], pairs_list, u.rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Separations for Visit %i\" % test_visit_2)\n",
    "visit_2_seps = calc_pairwise_separations(keep_visit_2['coord_ra'], keep_visit_2['coord_dec'], pairs_list, u.rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_seps = np.array([visit_1_seps, visit_2_seps]).T\n",
    "sep_df = pd.DataFrame(visit_seps, columns=['sep_visit_1', 'sep_visit_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_df['diff'] = sep_df['sep_visit_1'] - sep_df['sep_visit_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup `lsst_verify`\n",
    "\n",
    "Following `verify_demo` [notebook](https://github.com/LSSTScienceCollaborations/StackClub/blob/master/Validation/verify_demo.ipynb) we create a metric package for astrometry and call it `verify_astrometry`. In the json files we add our metrics and the design specs that are required for commissioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_PACKAGE = \"verify_astrometry\"\n",
    "metrics = lsst.verify.MetricSet.load_metrics_package(METRIC_PACKAGE)\n",
    "specs = lsst.verify.SpecificationSet.load_metrics_package(METRIC_PACKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test against requirements\n",
    "\n",
    "To show reports from `lsst_verify` we calculate the parameters we want to test and format them as `Measurement` objects with additional information saved as `Datum` objects so we can use to make diagnostic plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = 30 # Get 60 arcseconds on either side of defined separation\n",
    "five_arcmin = 60*5 # five arcminutes in arcseconds\n",
    "five_arcmin_df = sep_df.query('sep_visit_1 > %i-%i and sep_visit_1 < %i+%i' % (five_arcmin, lims,\n",
    "                                                                               five_arcmin, lims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_diff_5_arcmin = np.median(np.abs(five_arcmin_df['diff']))*1000*u.mas\n",
    "am1_meas = lsst.verify.Measurement('relative_astrometry.AM1', median_diff_5_arcmin)\n",
    "\n",
    "am1_meas.extras['meas_errors'] = lsst.verify.Datum(np.abs(five_arcmin_df['diff']).values*1000*u.mas,\n",
    "                                                   label='Differences in Measurement (mas)',\n",
    "                                                   description='Differences in measurements of pairs of sources on 5-arcmin. scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_frac_5_arcmin = len(np.where((np.abs(five_arcmin_df['diff'])*1000) > 20)[0]) / len(five_arcmin_df)\n",
    "af1_meas = lsst.verify.Measurement('relative_astrometry.AF1', outlier_frac_5_arcmin)\n",
    "af1_meas.extras['meas_errors'] = lsst.verify.Datum(np.abs(five_arcmin_df['diff']).values*1000*u.mas,\n",
    "                                                   label='Differences in Measurement (mas)',\n",
    "                                                   description='Differences in measurements of pairs of sources on 5-arcmin. scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = 30 # Get 60 arcseconds on either side of defined separation\n",
    "twenty_arcmin = 60*20 # twenty arcminutes in arcseconds\n",
    "twenty_arcmin_df = sep_df.query('sep_visit_1 > %i-%i and sep_visit_2 < %i+%i' % (twenty_arcmin, lims,\n",
    "                                                                           twenty_arcmin, lims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_diff_20_arcmin = np.median(np.abs(twenty_arcmin_df['diff']))*1000*u.mas\n",
    "am2_meas = lsst.verify.Measurement('relative_astrometry.AM2', median_diff_20_arcmin)\n",
    "\n",
    "am2_meas.extras['meas_errors'] = lsst.verify.Datum(np.abs(twenty_arcmin_df['diff']).values*1000*u.mas,\n",
    "                                                   label='Differences in Measurement (mas)',\n",
    "                                                   description='Differences in measurements of pairs of sources on 20-arcmin. scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_frac_20_arcmin = len(np.where((np.abs(twenty_arcmin_df['diff'])*1000) > 20)[0]) / len(twenty_arcmin_df)\n",
    "af2_meas = lsst.verify.Measurement('relative_astrometry.AF2', outlier_frac_20_arcmin)\n",
    "af2_meas.extras['meas_errors'] = lsst.verify.Datum(np.abs(twenty_arcmin_df['diff']).values*1000*u.mas,\n",
    "                                                   label='Differences in Measurement (mas)',\n",
    "                                                   description='Differences in measurements of pairs of sources on 20-arcmin. scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all values are calculated for metrics we add them to a `Job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = lsst.verify.Job(metrics=metrics, specs=specs)\n",
    "job.measurements.insert(am1_meas)\n",
    "job.measurements.insert(am2_meas)\n",
    "job.measurements.insert(af1_meas)\n",
    "job.measurements.insert(af2_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add available metadata to the job. This metadata can be used to differentiate tests of the same metrics in Squash. Here we add the bandpass as metadata, but we could also add in information like the dataset we are using to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta.update({'filter': '%s' % test_bandpass})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the job and print out a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.report().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push job results to Squash\n",
    "\n",
    "Here we push the results to the [Squash dashboard](chronograf-demo.lsst.codes/) so we can track measurements over time interactively.\n",
    "\n",
    "First we point at the api. Currently we are pushing our results to the sandbox database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_api_url = \"https://squash-restful-api-sandbox.lsst.codes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter credentials to get access. Only authenticated users can push to Squash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import getpass\n",
    "#username = getpass.getuser()\n",
    "#password = getpass.getpass(prompt='Password for user `{}`: '.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current hack to get CI working with notebooks and chronograf\n",
    "# Uses a password in a read-only file readable only by the user\n",
    "username = os.environ['USER']\n",
    "with open(os.path.join(os.environ['HOME'], 'bk_abc.txt')) as f:\n",
    "    password = f.readline()\n",
    "password = password[:-1] # Remove new line character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "credentials = {'username': username, 'password': password}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time you can register as a new user by uncommenting the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.post('{}/register'.format(squash_api_url), json=credentials)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/auth'.format(squash_api_url), json=credentials)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Authorization': 'JWT {}'.format(r.json()['access_token'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells upload the metric definitions to Squash and are a one-time setup procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/metrics'.format(squash_api_url),\n",
    "                json={'metrics': metrics.json},\n",
    "                headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/specs'.format(squash_api_url),\n",
    "                json={'specs': specs.json},\n",
    "                headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add some more metadata that is required for Squash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta.update({'packages': {}})\n",
    "job.meta.update({'env': {'env_name': 'jenkins'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we dispatch the results of the `Job` we ran to Squash and can view them on the Squash dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.dispatch(api_user=username, api_password=password, api_url=squash_api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results against requirements\n",
    "\n",
    "Pick out pairs with Gaia Separations of 5, 20, 200 arcminutes and compare differences in measured separation to compare against requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 arcmin tests\n",
    "\n",
    "1. For all pairs of sources separated by ~5 arcminutes median error in these measurements is <= 10 milliarcseconds.\n",
    "\n",
    "2. No more than 10% of the source pairs separated by ~5 arcminutes have separation errors greater than 20 milliarcseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.hist(am1_meas.extras['meas_errors'].quantity, range=(0, 100), bins=20)\n",
    "plt.axvline(am1_meas.quantity.value, 0, 1, \n",
    "            c='k', label='Median Difference: %.2f mas' % am1_meas.quantity.value, lw=4)\n",
    "thresh = specs['relative_astrometry.AM1.design'].threshold.value\n",
    "plt.axvline(thresh,\n",
    "            0, 1, c='r', label='Requirement = %.1f milliarcsec' % thresh,\n",
    "            lw=4)\n",
    "plt.legend()\n",
    "plt.xlabel('%s' % am1_meas.extras['meas_errors'].label)\n",
    "plt.ylabel('Number of Pairs')\n",
    "plt.title('%s' % am1_meas.extras['meas_errors'].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "bins = np.linspace(0, 100, 21)\n",
    "bins = np.append(bins, np.max(af1_meas.extras['meas_errors'].quantity.value)+0.01)\n",
    "n, bins, _ = plt.hist(af1_meas.extras['meas_errors'].quantity.value, bins=bins,\n",
    "                      cumulative=True, density=True)\n",
    "plt.xlim((0, 100))\n",
    "plt.axhline(1. - af1_meas.quantity.value, 0, 1, c='k', \n",
    "            label='Outlier Percentage = %.2f%s' % ((af1_meas.quantity.value)*100, '%'), \n",
    "            lw=4)\n",
    "plt.axhline(0.9, 0, 1, c='r', ls='--', label='Requirement: Outlier Fraction (> 20mas) <= 10%', lw=4)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('%s' % af1_meas.extras['meas_errors'].label)\n",
    "plt.ylabel('Cumulative Fraction of Pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20 arcmin tests\n",
    "\n",
    "3. For all pairs of sources separated by ~20 arcminutes median error in these measurements is <= 10 milliarcseconds.\n",
    "\n",
    "4. No more than 10% of the source pairs separated by ~20 arcminutes have separation errors greater than 20 milliarcseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.hist(am2_meas.extras['meas_errors'].quantity, range=(0, 100), bins=20)\n",
    "plt.axvline(am2_meas.quantity.value, 0, 1, \n",
    "            c='k', label='Median Difference: %.2f mas' % am2_meas.quantity.value, lw=4)\n",
    "thresh = specs['relative_astrometry.AM2.design'].threshold.value\n",
    "plt.axvline(thresh,\n",
    "            0, 1, c='r', label='Requirement = %.1f milliarcsec' % thresh,\n",
    "            lw=4)\n",
    "plt.legend()\n",
    "plt.xlabel('%s' % am2_meas.extras['meas_errors'].label)\n",
    "plt.ylabel('Number of Pairs')\n",
    "plt.title('%s' % am2_meas.extras['meas_errors'].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "bins = np.linspace(0, 100, 21)\n",
    "bins = np.append(bins, np.max(af2_meas.extras['meas_errors'].quantity.value)+0.01)\n",
    "n, bins, _ = plt.hist(af2_meas.extras['meas_errors'].quantity.value, bins=bins,\n",
    "                      cumulative=True, density=True)\n",
    "plt.xlim((0, 100))\n",
    "plt.axhline(1. - af2_meas.quantity.value, 0, 1, c='k', \n",
    "            label='Outlier Percentage = %.2f%s' % ((af2_meas.quantity.value)*100, '%'), \n",
    "            lw=4)\n",
    "plt.axhline(0.9, 0, 1, c='r', ls='--', label='Requirement: Outlier Fraction (> 20mas) <= 10%', lw=4)\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('%s' % af2_meas.extras['meas_errors'].label)\n",
    "plt.ylabel('Cumulative Fraction of Pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200 arcmin tests\n",
    "\n",
    "5. For all pairs of sources separated by ~200 arcminutes median error in these measurements is <= 15 milliarcseconds.\n",
    "\n",
    "6. No more than 10% of the source pairs separated by ~200 arcminutes have separation errors greater than 30 milliarcseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lims = 30 # Get 30 arcseconds on either side of defined separation\n",
    "# two_hundred_arcmin = 60*200 # 200 arcminutes in arcseconds\n",
    "# two_hundred_arcmin_df = sep_df.query('sep_gaia > %i-%i and sep_gaia < %i+%i' % (two_hundred_arcmin, \n",
    "#                                                                                 lims,\n",
    "#                                                                                 two_hundred_arcmin, \n",
    "#                                                                                 lims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# median_diff_200_arcmin = np.median(np.abs(two_hundred_arcmin_df['diff']))\n",
    "# plt.hist(np.abs(two_hundred_arcmin_df['diff']), range=(0, 0.1), bins=20)\n",
    "# plt.axvline(median_diff_200_arcmin, 0, 1, \n",
    "#             c='k', label='Median Difference: %.2f mas' % (median_diff_200_arcmin*1000), lw=4)\n",
    "# plt.axvline(0.015, 0, 1, c='r', label='Requirement = 15 milliarcsec', lw=4)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Difference in Measured Separation for sources separated by ~200 arcmin (arcsec)')\n",
    "# plt.ylabel('Number of Pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# n, bins, _ = plt.hist(np.abs(two_hundred_arcmin_df['diff']), range=(0, 0.1), bins=20,\n",
    "#                       cumulative=True, density=True)\n",
    "# current_outlier_frac_200 = n[np.where(bins < 0.02)[0][-1]]\n",
    "# plt.axhline(current_outlier_frac_200, 0, 1, c='k', \n",
    "#             label='Outlier Percentage = %.2f%s' % ((1.-current_outlier_frac_200)*100, '%'), \n",
    "#             lw=4)\n",
    "# plt.axhline(0.9, 0, 1, c='r', ls='--', label='Requirement: Outlier Fraction (> 30mas) <= 10%', lw=4)\n",
    "# plt.legend(loc=4)\n",
    "# plt.xlabel('Difference in Measured Separation for sources separated by ~200 arcmin (arcsec)')\n",
    "# plt.ylabel('Cumulative Fraction of Pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
