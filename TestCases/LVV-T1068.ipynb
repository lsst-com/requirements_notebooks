{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVV-T1068: Coaddition for Deep Detection\n",
    "\n",
    "**Written By: Bryce Kalmbach**\n",
    "\n",
    "**Last updated: 10-03-2019**\n",
    "\n",
    "**Tested on Stack Version: w_2019_38**\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "[OSS-REQ-0157](https://docushare.lsst.org/docushare/dsweb/Get/LSE-030#page=60)\n",
    "\n",
    "1. Fraction of all detections on deep detection coadds caused by unremoved artifacts shall not exceed 0.1%\n",
    "\n",
    "## Proposed Test Case:\n",
    "\n",
    "1. We will obtain detection catalogs obtained from the ComCam/LSSTCam deep co-addition, and compare them to external data sets (such as HSC, HST, DLS, or CFHTLens).  Sources in the ComCam/LSSTCam dataset which have no corresponding detections in the external data will be considered false detections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import lsst.verify\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from lsst.daf.persistence import Butler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for testing\n",
    "\n",
    "* `test_bandpass`: The notebook will set up to test catalogs in this bandpass\n",
    "\n",
    "* `mag_lims`: Keep detections with magnitudes in between `[bright_limit, faint_limit]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bandpass = 'HSC-R'\n",
    "\n",
    "mag_lims = [20., 26.] \n",
    "# According to survey info: https://hsc.mtk.nao.ac.jp/ssp/survey/, HSC-R goes down to 27.1 in DEEP when finished\n",
    "# We will set faint limit at 26.0 for testing currently since HSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load butler for HSC Deep and Ultra-Deep `deepCoadd`\n",
    "\n",
    "We will compare the coadd source catalogs from the HSC Ultra-Deep data to\n",
    "the coadd source catalog from HSC Deep. Since the Ultra-Deep fields\n",
    "are completely within the HSC Deep footprint we can match sources in the Ultra-Deep\n",
    "catalogs and know we are covered within the Deep footprint. Then if we compare\n",
    "detection results as a function of magnitude we don't have to worry about the\n",
    "varying magnitude depth in the surveys as long as we stay within the magnitude\n",
    "limit of the HSC Deep field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_repo_dir = '/datasets/hsc/repo/rerun/DM-13666/DEEP'\n",
    "deep_butler = Butler(deep_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_deep_repo_dir = '/datasets/hsc/repo/rerun/DM-13666/UDEEP'\n",
    "u_deep_butler = Butler(u_deep_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Gen 3 butler need to go into filesystem to get list of available tracts\n",
    "deep_tracts = os.listdir(os.path.join(deep_repo_dir, 'deepCoadd', test_bandpass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_deep_tracts = os.listdir(os.path.join(u_deep_repo_dir, 'deepCoadd', test_bandpass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the tract numbers that overlap and then identify the specific patches in each tract that overlap between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_tracts = list(set(deep_tracts).intersection(set(u_deep_tracts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_patches = {}\n",
    "for tract in overlap_tracts:\n",
    "    deep_patches = os.listdir(os.path.join(deep_repo_dir, 'deepCoadd', test_bandpass, tract))\n",
    "    u_deep_patches = os.listdir(os.path.join(u_deep_repo_dir, 'deepCoadd', test_bandpass, tract))\n",
    "    common_patches = []\n",
    "    for deep_patch in deep_patches:\n",
    "        # only want folder names since these are the actual patch ids\n",
    "        if len(deep_patch.split('.')) == 1:\n",
    "            if deep_patch in u_deep_patches:\n",
    "                common_patches.append(deep_patch)\n",
    "    overlap_patches[tract] = common_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the source catalogs from a given tract for comparison\n",
    "We pick a random tract and loop over all patches in that tract getting all the\n",
    "sources from each catalog and compiling them in a pandas dataframe.\n",
    "\n",
    "#### Only keep deep sources within ultradeep field\n",
    "\n",
    "There are two ultradeep fields and HSC has a diameter of 1.5 degrees. Thus, keep only sources within 0.5 degrees of the center of each field to make sure we are not taking deep sources outside of the ultradeep area when we try to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_1_center = SkyCoord('02h18m00s -5', unit=(u.hourangle, u.deg))\n",
    "ud_2_center = SkyCoord('10h00m29s +2d12m21s', unit=(u.hourangle, u.deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = []\n",
    "for tract_on in overlap_tracts:\n",
    "    num_patches.append(len(overlap_patches[tract_on]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick tract with most overlap patches\n",
    "test_tract = overlap_tracts[np.argmax(num_patches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_src_df = None\n",
    "u_deep_src_df = None\n",
    "for test_patch_idx in range(len(overlap_patches[test_tract])):\n",
    "    print(overlap_patches[test_tract][test_patch_idx])\n",
    "    deep_src = deep_butler.get('deepCoadd_forced_src', tract=int(test_tract), \n",
    "                               patch=overlap_patches[test_tract][test_patch_idx], \n",
    "                               filter=test_bandpass)\n",
    "    u_deep_src = u_deep_butler.get('deepCoadd_forced_src', tract=int(test_tract), \n",
    "                                   patch=overlap_patches[test_tract][test_patch_idx], \n",
    "                                   filter=test_bandpass)\n",
    "    deep_photo_calib = deep_butler.get('deepCoadd_photoCalib', tract=int(test_tract), \n",
    "                                       patch=overlap_patches[test_tract][test_patch_idx], \n",
    "                                       filter=test_bandpass)\n",
    "    ud_photo_calib = u_deep_butler.get('deepCoadd_photoCalib', tract=int(test_tract), \n",
    "                                       patch=overlap_patches[test_tract][test_patch_idx], \n",
    "                                       filter=test_bandpass)\n",
    "    deep_flux = deep_src['base_PsfFlux_instFlux']\n",
    "    deep_mags = deep_photo_calib.instFluxToMagnitude(deep_src, 'base_PsfFlux')\n",
    "    u_deep_mags = ud_photo_calib.instFluxToMagnitude(u_deep_src, 'base_PsfFlux')\n",
    "    \n",
    "    if u_deep_src_df is None:\n",
    "        u_deep_src_df = u_deep_src.asAstropy().to_pandas()\n",
    "        u_deep_src_df = u_deep_src_df[['coord_ra', 'coord_dec', 'deblend_nChild']]\n",
    "        u_deep_src_df['mag'] = u_deep_mags[:, 0]\n",
    "        u_deep_src_df['mag_err'] = u_deep_mags[:, 1]\n",
    "        \n",
    "        u_deep_sample = SkyCoord(np.degrees(u_deep_src_df['coord_ra'])*u.deg, \n",
    "                                 np.degrees(u_deep_src_df['coord_dec'])*u.deg)\n",
    "\n",
    "        # Identify field 1 or 2\n",
    "        if u_deep_sample[0].separation(ud_1_center) < 2*u.deg:\n",
    "            ud_center = ud_1_center\n",
    "        else:\n",
    "            ud_center = ud_2_center\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        temp_ud_src_df = u_deep_src.asAstropy().to_pandas()\n",
    "        temp_ud_src_df = temp_ud_src_df[['coord_ra', 'coord_dec', 'deblend_nChild']]\n",
    "        temp_ud_src_df['mag'] = u_deep_mags[:, 0]\n",
    "        temp_ud_src_df['mag_err'] = u_deep_mags[:, 1]\n",
    "        u_deep_src_df = pd.concat([u_deep_src_df, temp_ud_src_df], sort=False)\n",
    "\n",
    "    if deep_src_df is None:\n",
    "        deep_src_df = deep_src.asAstropy().to_pandas()\n",
    "        deep_src_df = deep_src_df[['coord_ra', 'coord_dec', 'deblend_nChild']]\n",
    "        \n",
    "        deep_coords = SkyCoord(np.degrees(deep_src_df['coord_ra'])*u.deg, \n",
    "                               np.degrees(deep_src_df['coord_dec'])*u.deg)\n",
    "        deep_sep = deep_coords.separation(ud_center)\n",
    "        deep_keep = np.where(deep_sep <= 0.75*u.deg)\n",
    "        deep_coords = deep_coords[deep_keep]\n",
    "        deep_src_df = deep_src_df.iloc[deep_keep].reset_index(drop=True)\n",
    "        \n",
    "        deep_src_df['mag'] = deep_mags[deep_keep[0], 0]\n",
    "        deep_src_df['mag_err'] = deep_mags[deep_keep[0], 1]\n",
    "\n",
    "    else:\n",
    "        temp_deep_src_df = deep_src.asAstropy().to_pandas()\n",
    "        temp_deep_src_df = temp_deep_src_df[['coord_ra', 'coord_dec', 'deblend_nChild']]\n",
    "\n",
    "        deep_coords = SkyCoord(np.degrees(temp_deep_src_df['coord_ra'])*u.deg, \n",
    "                               np.degrees(temp_deep_src_df['coord_dec'])*u.deg)\n",
    "        deep_sep = deep_coords.separation(ud_center)\n",
    "        deep_keep = np.where(deep_sep <= 0.75*u.deg)\n",
    "        deep_coords = deep_coords[deep_keep]\n",
    "        temp_deep_src_df = temp_deep_src_df.iloc[deep_keep].reset_index(drop=True)\n",
    "        \n",
    "        temp_deep_src_df['mag'] = deep_mags[deep_keep[0], 0]\n",
    "        temp_deep_src_df['mag_err'] = deep_mags[deep_keep[0], 1]\n",
    "        deep_src_df = pd.concat([deep_src_df, temp_deep_src_df], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove entries for deblended parent objects to avoid double counting\n",
    "Since the parent objects that are deblended into child objects \n",
    "are included in the source catalogs we exclude them in the rest of the analysis\n",
    "to avoid double counting.\n",
    "\n",
    "Also remove any sources with magnitudes outside the range we specify at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_src_df = deep_src_df.query('deblend_nChild == 0 and mag >= %f and mag <= %f' % tuple(mag_lims)).reset_index(drop=True)\n",
    "u_deep_src_df = u_deep_src_df.query('deblend_nChild == 0').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use astropy to spatially match the catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_coords = SkyCoord(np.degrees(deep_src_df['coord_ra'])*u.deg, \n",
    "                       np.degrees(deep_src_df['coord_dec'])*u.deg)\n",
    "u_deep_coords = SkyCoord(np.degrees(u_deep_src_df['coord_ra'])*u.deg, \n",
    "                         np.degrees(u_deep_src_df['coord_dec'])*u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that deep coverage does span ultra-deep coverage\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.scatter(deep_coords.ra.deg, deep_coords.dec.deg, label='deep')\n",
    "plt.scatter(u_deep_coords.ra.deg, u_deep_coords.dec.deg, alpha=0.05, label='u_deep')\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1.0)\n",
    "plt.xlabel('ra')\n",
    "plt.ylabel('dec')\n",
    "plt.title('Coverage map: Deep vs Ultra-Deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_idx, d2d, d3d = deep_coords.match_to_catalog_sky(u_deep_coords)\n",
    "max_sep = 0.5*u.arcsec\n",
    "# Reject results outside max separation\n",
    "matched_idx[np.where(d2d > max_sep)] = -99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to only take the closest match to something in the reference catalog\n",
    "unique_idx, idx_counts = np.unique(matched_idx, return_counts=True)\n",
    "for idx_match, idx_count in zip(unique_idx, idx_counts):\n",
    "    if idx_match == -99:\n",
    "        continue\n",
    "    elif idx_count == 1:\n",
    "        continue\n",
    "    else:\n",
    "        duplicate_deep_idx = np.where(matched_idx == idx_match)[0]\n",
    "        duplicate_distances = d2d[duplicate_deep_idx]\n",
    "        min_dist_idx = np.argsort(duplicate_distances)\n",
    "        matched_idx[duplicate_deep_idx[min_dist_idx[1:]]] = -99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_objects_idx = np.where(matched_idx > -1)[0]\n",
    "unmatched_idx = np.where(matched_idx < -1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positive fraction is 1 minus the fraction of matched deep catalog objects over total objects in the catalog.\n",
    "\n",
    "$false\\ positive\\ fraction = 1.0 - \\frac{matched\\ deep\\ objects}{total\\ deep\\ objects}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_frac = 1. - len(found_objects_idx)/len(deep_coords)\n",
    "print(false_positive_frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup `lsst_verify`\n",
    "\n",
    "We have a metric package for catalogs called `verify_catalogs`. In the json files we add our metrics and the design specs that are required for commissioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_PACKAGE = \"verify_catalogs\"\n",
    "metrics = lsst.verify.MetricSet.load_metrics_package(METRIC_PACKAGE)\n",
    "specs = lsst.verify.SpecificationSet.load_metrics_package(METRIC_PACKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test against requirements\n",
    "\n",
    "To show reports from `lsst_verify` we calculate the parameters we want to test and format them as `Measurement` objects with additional information saved as `Datum` objects so we can use to make diagnostic plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falseDeepDetect_meas = lsst.verify.Measurement('coadd_detection.falseDeepDetect',\n",
    "                                               false_positive_frac)\n",
    "falseDeepDetect_meas.extras['matched_detection_mags'] = lsst.verify.Datum(deep_src_df.iloc[found_objects_idx]['mag'], unit=u.mag,\n",
    "                                                                         label='Matched Detection Magnitudes',\n",
    "                                                                         description='Magnitudes of rows in Deep catalog matched to Ultradeep objects')\n",
    "falseDeepDetect_meas.extras['deep_mags'] = lsst.verify.Datum(deep_src_df['mag'], unit=u.mag,\n",
    "                                                            label='Deep Catalog Mags',\n",
    "                                                            description='Magnitudes of sources in deep catalog')\n",
    "falseDeepDetect_meas.extras['matched_ra'] = lsst.verify.Datum(deep_coords.ra.deg[found_objects_idx], unit=u.deg,\n",
    "                                                                        label='Matched RA',\n",
    "                                                                        description='RA of matched deep catalog objects')\n",
    "falseDeepDetect_meas.extras['matched_dec'] = lsst.verify.Datum(deep_coords.dec.deg[found_objects_idx], unit=u.deg,\n",
    "                                                                        label='Matched Dec',\n",
    "                                                                        description='Dec of matched deep catalog objects')\n",
    "falseDeepDetect_meas.extras['unmatched_ra'] = lsst.verify.Datum(deep_coords.ra.deg[unmatched_idx], unit=u.deg,\n",
    "                                                                        label='Unmatched RA',\n",
    "                                                                        description='RA of unmatched deep catalog objects')\n",
    "falseDeepDetect_meas.extras['unmatched_dec'] = lsst.verify.Datum(deep_coords.dec.deg[unmatched_idx], unit=u.deg,\n",
    "                                                                        label='Unmatched Dec',\n",
    "                                                                        description='Dec of unmatched deep catalog objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all values are calculated for metrics we add them to a `Job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = lsst.verify.Job(metrics=metrics, specs=specs)\n",
    "job.measurements.insert(falseDeepDetect_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add available metadata to the job. This metadata can be used to differentiate tests of the same metrics in Squash. Here we add the bandpass as metadata, but we could also add in information like the dataset we are using to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta.update({'filter': '%s' % test_bandpass})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the job and print out a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.report().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push job results to Squash\n",
    "\n",
    "Here we push the results to the [Squash dashboard](chronograf-demo.lsst.codes/) so we can track measurements over time interactively.\n",
    "\n",
    "First we point at the api. Currently we are pushing our results to the sandbox database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squash_api_url = \"https://squash-restful-api-sandbox.lsst.codes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter credentials to get access. Only authenticated users can push to Squash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import getpass\n",
    "#username = getpass.getuser()\n",
    "#password = getpass.getpass(prompt='Password for user `{}`: '.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current hack to get CI working with notebooks and chronograf\n",
    "# Uses a password in a read-only file readable only by the user\n",
    "username = os.environ['USER']\n",
    "with open(os.path.join(os.environ['HOME'], 'bk_abc.txt')) as f:\n",
    "    password = f.readline()\n",
    "password = password[:-1] # Remove new line character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "credentials = {'username': username, 'password': password}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time you can register as a new user by uncommenting the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.post('{}/register'.format(squash_api_url), json=credentials)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/auth'.format(squash_api_url), json=credentials)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Authorization': 'JWT {}'.format(r.json()['access_token'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells upload the metric definitions to Squash and are a one-time setup procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/metrics'.format(squash_api_url),\n",
    "                json={'metrics': metrics.json},\n",
    "                headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('{}/specs'.format(squash_api_url),\n",
    "                json={'specs': specs.json},\n",
    "                headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add some more metadata that is required for Squash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta.update({'packages': {}})\n",
    "job.meta.update({'env': {'env_name': 'jenkins'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we dispatch the results of the `Job` we ran to Squash and can view them on the Squash dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.dispatch(api_user=username, api_password=password, api_url=squash_api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the detections between the catalogs\n",
    "We now compare the number of detections matched between the Ultra-Deep catalog\n",
    "and the Deep catalog to the total number of detections in the Ultra-Deep catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "n, bins, _ = plt.hist(falseDeepDetect_meas.extras['matched_detection_mags'].quantity, \n",
    "                      histtype='step', range=mag_lims, bins=np.int(mag_lims[1] - mag_lims[0])*4,\n",
    "                      label='Matched Detections to Ultra-Deep Catalog', lw=3)\n",
    "n2, bins, _ = plt.hist(falseDeepDetect_meas.extras['deep_mags'].quantity, histtype='step', \n",
    "                       bins=bins, label='Total Deep Detections', lw=3)\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Number of detections')\n",
    "plt.title('Number of Detections in Deep catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_spacing = bins[1]-bins[0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.plot(bins[:-1]+bin_spacing/2., 1.-n/n2, lw=3, marker='o', markersize=10)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('1.0 - (matched/total detections)')\n",
    "false_positive_frac = 1 - np.sum(n)/np.sum(n2)\n",
    "plt.title('False Positive Ratio by Magnitude. Overall False Pos. Fraction = %.2f' % false_positive_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.scatter(falseDeepDetect_meas.extras['unmatched_ra'].quantity, \n",
    "            falseDeepDetect_meas.extras['unmatched_dec'].quantity)\n",
    "plt.title('Unmatched objects by location')\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('Dec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
